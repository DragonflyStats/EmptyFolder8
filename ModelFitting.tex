
Week 11

Wednesday April 2012


Outcomes

Linear Regression Models

•
Simple Linear Regression Models

•
Multiple Linear Regression Models

•
Influence and Outliers



The Line of Best Fit

Note that the data in the above scatter-plot is approximately linear. As the age increases, the average height increases at an approximately constant rate.

One could not fit a single line through each and every data point, but one could imagine a line that is fairly close to each data point, with some of the data points appearing above the line, others below for balance. 

In this section, we will calculate and plot the Line of Best Fit, or the Least Squares Regression Line.


We will use R's lm command to compute a "linear model" that fits the data in the scatterplot.

The command lm is a very sophisticated command with a host of options (type ?lm to view a full description), but in its simplest form, it is quite easy to use. 


The syntax height~age is called a model equation and is a very sophisticated R construct. 

We are using its most simple form here.

The symbol separating "height" and "age" in the syntax height~age is a "tilde."


It is located on the key to the immediate left of the the #1 key on your keyboard.

You must use the Shift Key to access the "tilde."



--------------------------------------------------------------------------------


> res=lm(height~age)



Let's examine what is returned in the variable res.

> res

Call:
lm(formula = height ~ age)

Coefficients:
(Intercept)          age  
     64.928        0.635  



Note the "Coefficients" part of the contents of res. These coefficients are the intercept and slope of the line of best fit. Essentially, we are being told that the equation of the line of best fit is:

height = 0.635 age + 64.928.

Note that this result has the form y = m x + b, where m is the slope and b is the intercept of the line.

It is a simple matter to superimpose the "Line of Best Fit" provided by the contents of the variable res. The command abline will use the data in res to draw the "Line of Best Fit."


> abline(res)



The result of this command is the "Line of Best Fit" shown in Figure 2.




--------------------------------------------------------------------------------



The estimates (i.e regression coefficients) may be determined directly using the

coef() command.


Investigate this using the RESID command.

.

The basic function for fitting ordinary model is lm().

Syntax

Fitted.Model = lm(formula, data=data.frame)

The R command summary() provides a comprehensive summary of the results of the regression analysis.

Updating fitted models

The update function is a convenient function that allows a previously fitted model to be altered with new specifications.







--------------------------------------------------------------------------------



Useful Commands

The estimates (i.e regression coefficients) may be determined directly using the coef() command. coef(fit1) returns the intercept and slope estimates for the model.


The summary function provides a comprehensive overview of all inference estimates for a model fit, including p-values for all predictor variables.


The summary output is constructed as a list, and as such, components can be aaccessed using the dollar sign.(Find the names of the component using the names command)

Residuals

Residuals are normally distributed with mean zero. 

Investigate this using the RESID command.

Testing Residuals for Normality


A fundamental assumption of linear models is that the

Residuals are normally distributed with mean zero.


The Anderson Darling test is the conventional inference test for assessing whether a data set is normally distributed. It requires liading the normtest pagkage into the R environment. A nother inference procedure for testing normality is the Shapiro Wilk test. (We will use this one fir this module, but use Anderson Darling in future).


shapiro.test(x)





--------------------------------------------------------------------------------


Model Selection


The adjusted coefficient of determination is computed to account for the presence of more than one predictor variable.


The law of parsimony, the simplest model that adequately explains the outcomes is the best.




--------------------------------------------------------------------------------


